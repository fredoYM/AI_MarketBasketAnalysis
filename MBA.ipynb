{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664a756c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2acc3",
   "metadata": {},
   "source": [
    "This program deals with Market Basket Analysis using a dataset and some operations will be performed on it to find insights and use the resultant data to find more inferences for future decision making and better sales opportunities.\n",
    "\n",
    "Market Basket Analysis is a powerful technique that allows us to uncover patterns and associations between items that customers tend to purchase together. By analyzing these patterns, we can gain valuable insights that can drive business decisions and strategies.\n",
    "\n",
    "In this notebook, we will work with a Market Basket dataset that captures customer transactions in a retail or e-commerce setting. The dataset provides a wealth of information about customer purchases, allowing us to dive deep into their buying behavior. By leveraging data mining techniques and association rule mining algorithms, we will unravel the relationships between items and discover interesting patterns.\n",
    "\n",
    "Through this analysis, we can derive actionable insights to improve various aspects of business operations. We can identify frequently co-purchased items, enabling us to make targeted product recommendations and enhance cross-selling and upselling opportunities. By optimizing product placement and store layout based on association patterns, we can create more enticing shopping experiences. Furthermore, we can design effective promotional campaigns by leveraging the discovered item associations, resulting in higher customer engagement and increased sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5f917",
   "metadata": {},
   "source": [
    "# Overview of the Market Basket Analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558655c2",
   "metadata": {},
   "source": [
    "This dataset contains 522,065 rows and 7 attributes that provide valuable information about customer transactions and product details. Here is a breakdown of the attributes:\n",
    "\n",
    "    BillNo: This attribute represents a 6-digit number assigned to each transaction. It serves as a unique identifier for identifying individual purchases.\n",
    "\n",
    "    Itemname: This attribute stores the name of the product purchased in each transaction. It provides nominal data representing different products.\n",
    "\n",
    "    Quantity: This attribute captures the quantity of each product purchased in a transaction. It is a numeric value that indicates the number of units of a specific item.\n",
    "\n",
    "    Date: The Date attribute records the day and time when each transaction occurred. It provides valuable information about the timing of purchases.\n",
    "\n",
    "    Price: This attribute represents the price of each product. It is a numeric value that indicates the cost of a single unit of the item.\n",
    "\n",
    "    CustomerID: Each customer is assigned a 5-digit number as their unique identifier. This attribute helps track customer-specific information and analyze individual buying patterns.\n",
    "\n",
    "    Country: The Country attribute denotes the name of the country where each customer resides. It provides nominal data representing different geographic regions.\n",
    "\n",
    "By analyzing this dataset, we can gain insights into customer purchasing behavior, identify popular products, examine sales trends over time, and explore the impact of factors such as price and geography on customer preferences. These insights can be used to optimize marketing strategies, improve inventory management, and enhance customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf024a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490f310",
   "metadata": {},
   "source": [
    "` Importing the data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ebdac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy library for efficient array operations\n",
    "import pandas as pd  # Import pandas library for data processing\n",
    "import matplotlib.pyplot as plt  # Import matplotlib.pyplot for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e15b4f",
   "metadata": {},
   "source": [
    "` Loading the data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Assignment-1_Data.csv', sep=';',parse_dates=['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efa78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Price' column to float64 data type after replacing commas with dots\n",
    "df['Price'] = df['Price'].str.replace(',', '.').astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff3ddb",
   "metadata": {},
   "source": [
    "` Display information about the dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ec612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the information about the DataFrame which is to provide an overview of the DataFrame's structure and column data types.\n",
    "df.info()\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e3d9f",
   "metadata": {},
   "source": [
    "` Finding the number of missing values in each attribute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eca9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of missing values for each column and sort them in descending order\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31425c",
   "metadata": {},
   "source": [
    "` Calculate the Total Price value for data processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total price by multiplying the quantity and price columns\n",
    "df['Total_Price'] = df.Quantity * df.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of unique countries in the 'Country' column\n",
    "print(\"Number of unique countries:\", df['Country'].nunique())\n",
    "\n",
    "# Calculate and print the normalized value counts of the top 5 countries in the 'Country' column\n",
    "print(df['Country'].value_counts(normalize=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fcc8f",
   "metadata": {},
   "source": [
    "Considering that the majority of transactions (approximately 93%) in the dataset originate from the UK, the 'Country' column may not contribute significant diversity or variability to the analysis. Therefore, we can choose to remove the 'Country' column from the DataFrame df. we indicate that we want to drop a column, This step allows us to focus on other attributes that may provide more valuable insights for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4176874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the 'Country' column from the DataFrame\n",
    "df.drop('Country', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df23a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to display rows where 'BillNo' column contains non-digit values\n",
    "df[df['BillNo'].str.isdigit() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c405788",
   "metadata": {},
   "source": [
    "Since the item name \"Adjust bad debt\" was filled accidentally and does not provide any useful information for our analysis, we can choose to remove the corresponding rows from the DataFrame. The code snippet above filters the DataFrame df to retain only the rows where the 'Itemname' column does not contain the value \"Adjust bad debt\". This operation effectively eliminates the rows associated with the accidental data entry, ensuring the dataset is free from this irrelevant item name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8177d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'Itemname' column contains \"Adjust bad debt\"\n",
    "df = df[df['Itemname'] != \"Adjust bad debt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here to check if all BillNo doesn't inculde letters \n",
    "df['BillNo'].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dc0d9",
   "metadata": {},
   "source": [
    "` Excluding rows where 'Itemname' attribute is missing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows where 'Itemname' is missing (not NaN)\n",
    "df = df[df['Itemname'].notna()]\n",
    "\n",
    "# Print the number of unique items in the 'Itemname' column\n",
    "print(\"Number of unique items:\", df['Itemname'].nunique())\n",
    "\n",
    "# Calculate and print the normalized value counts of the top 5 items in the 'Itemname' column\n",
    "print(df['Itemname'].value_counts(normalize=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb3956",
   "metadata": {},
   "source": [
    "` Removing rows where 'Quantity' and 'Price' are zero`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9945bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Quantity' is less than 1\n",
    "df = df[df['Quantity'] >= 1]\n",
    "\n",
    "# Remove rows where the price is zero\n",
    "df = df[df['Price'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61063fb",
   "metadata": {},
   "source": [
    "# Data Understanding: Interpretation of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f2d53",
   "metadata": {},
   "source": [
    "In the data analysis process, data understanding plays a crucial role in gaining insights and formulating meaningful conclusions. By thoroughly examining the dataset, we aim to understand its structure, contents, and underlying patterns. This understanding empowers us to make informed decisions regarding data cleaning, feature engineering, and subsequent analysis steps.\n",
    "\n",
    "Key aspects of data understanding include:\n",
    "\n",
    "    Exploring the Dataset: We investigate the dataset's dimensions, such as the number of rows and columns, to gauge its size and complexity. Additionally, we examine the data types of each column to understand the nature of the variables.\n",
    "\n",
    "    Assessing Data Quality: We scrutinize the data for inconsistencies, outliers, or other data quality issues that may require attention. Addressing these issues ensures the reliability and accuracy of the data.\n",
    "\n",
    "    Identifying Relationships: We analyze the relationships between variables by examining correlations, associations, or dependencies. This analysis allows us to uncover meaningful connections that can drive insights and guide our analysis.\n",
    "\n",
    "    Detecting Patterns and Trends: We look for recurring patterns, trends, or distributions within the data. This step can reveal valuable information about customer behavior, market dynamics, or other relevant factors.\n",
    "\n",
    "By thoroughly understanding the dataset, we lay the foundation for meaningful data analysis and generate insights that contribute to informed decision-making and problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da29c4e",
   "metadata": {},
   "source": [
    "` Sorting most sold products by quantity and count and plotting a graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85928e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 10 most sold products by quantity\n",
    "df.groupby('Itemname')['Quantity'].sum().sort_values(ascending=False)[:10].plot(kind='barh', title='Number of Quantity Sold')\n",
    "plt.ylabel('Item Name')\n",
    "plt.xlim(20000, 82000)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the top 10 most sold products by count\n",
    "df['Itemname'].value_counts(ascending=False)[:10].plot(kind='barh', title='Number of Sales')\n",
    "plt.ylabel('Item Name')\n",
    "plt.xlim(1000, 2300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688565c",
   "metadata": {},
   "source": [
    "# Assocation Rules using Apriori algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f29b8",
   "metadata": {},
   "source": [
    "Association rules are generated using the Apriori algorithm, which is a popular algorithm for discovering interesting relationships or associations among items in a dataset. Association rule mining is commonly used in market basket analysis, where the goal is to find associations between items frequently purchased together.\n",
    "\n",
    "The association rules are evaluated based on different metrics, such as support, confidence, lift, leverage, and conviction. These metrics provide measures of the interestingness or strength of the rules. \n",
    "\n",
    "- Support measures the proportion of transactions in the dataset that contain both the antecedent and the consequent.\n",
    "- Confidence measures the conditional probability of the consequent given the antecedent.\n",
    "- Lift measures the ratio of observed support to expected support, indicating the strength of the association between the antecedent and the consequent.\n",
    "- Leverage measures the difference between the observed support and the expected support, indicating the significance of the association.\n",
    "- Conviction measures the ratio of the expected confidence to the observed confidence, indicating the degree of dependency between the antecedent and the consequent.\n",
    "\n",
    "By examining the association rules, you can identify interesting relationships, co-occurrences, or patterns among items, which can be used for various purposes such as product recommendation, market segmentation, or inventory management.\n",
    "\n",
    "To generate the association rules, we use the Apriori algorithm with a minimum support threshold of 0.05 (5%). This ensures that only itemsets with sufficient frequency in the dataset are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d412417",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Alfred\\User\\Code\\AI_MarketBasketAnalysis\\MBA.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Alfred/User/Code/AI_MarketBasketAnalysis/MBA.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assign the original DataFrame to df2\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Alfred/User/Code/AI_MarketBasketAnalysis/MBA.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df2 \u001b[39m=\u001b[39m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Assign the original DataFrame to df2\n",
    "df2 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c9f69",
   "metadata": {},
   "source": [
    "#### Filtering is done based on item occurrences:\n",
    "        The frequency count of each unique item name in the 'Itemname' column is calculated and stored in item_counts.\n",
    "        filtered_items is created by filtering item_counts to retain only item names that occur more than once.\n",
    "        Rows in df2 are filtered to keep only those where the item name in the 'Itemname' column is present in the filtered_items list.\n",
    "#### Filtering is done based on bill number occurrences:\n",
    "        The frequency count of each unique bill number in the 'BillNo' column is calculated and stored in bill_counts.\n",
    "        filtered_bills is created by filtering bill_counts to retain only bill numbers that occur more than once.\n",
    "        Rows in df2 are filtered to keep only those where the bill number in the 'BillNo' column is present in the filtered_bills list.\n",
    "\n",
    "After executing the code, the filtered DataFrame df2 will contain only the rows where both the item name and bill number occur more than once in the original df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table using the filtered DataFrame\n",
    "pivot_table = pd.pivot_table(df2[['BillNo','Itemname']], index='BillNo', columns='Itemname', aggfunc=lambda x: True, fill_value=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e53b8d",
   "metadata": {},
   "source": [
    "The code creates a pivot table that represents the occurrence of items in bills. The pivot table provides a binary representation where each cell indicates whether a specific item appears in a particular bill. Here's how it works:\n",
    "\n",
    "    The original DataFrame df2 contains information about bills and corresponding item names.\n",
    "    By using the pd.pivot_table() function, we reshape the DataFrame to create a pivot table.\n",
    "    The pivot table has 'BillNo' as the index and 'Itemname' as the columns, grouping the data based on these two columns.\n",
    "    The goal is to determine whether a specific item appears in a particular bill.\n",
    "    Each cell in the pivot table is filled with either True or False:\n",
    "        If an item appears in a bill, the corresponding cell is marked as True.\n",
    "        If an item does not appear in a bill, the corresponding cell is marked as False.\n",
    "    This binary representation of item occurrence in bills allows us to easily analyze and identify patterns or associations between different items and bills.\n",
    "\n",
    "The resulting pivot table provides a concise summary of the occurrence of items in bills, which can be used for various purposes such as market basket analysis, recommendation systems, or identifying frequent itemsets and association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Generate frequent itemsets with minimum support of 0.1 (10%)\n",
    "frequent_itemsets = apriori(pivot_table, min_support=0.01,use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, \"confidence\", min_threshold = 0.5)\n",
    "\n",
    "# Print frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Print association rules\n",
    "print(\"\\nAssociation Rules:\")\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1871087",
   "metadata": {},
   "source": [
    "The code uses the apriori algorithm and association rule mining techniques to analyze the occurrence of items in bills. Here's the overall idea:\n",
    "\n",
    "    Frequent Itemsets Generation:\n",
    "        The apriori algorithm is applied to the pivot_table created earlier, which represents the occurrence of items in bills.\n",
    "        The algorithm identifies sets of items that frequently co-occur together in the bills.\n",
    "        The minimum support threshold of 0.01 (1%) is set, meaning that an itemset must occur in at least 1% of the bills to be considered frequent.\n",
    "        The resulting frequent itemsets represent combinations of items that are frequently observed together in bills.\n",
    "\n",
    "    Association Rules Generation:\n",
    "        Using the frequent itemsets, association rules are generated.\n",
    "        Association rules capture relationships and patterns between items based on their co-occurrence in bills.\n",
    "        The confidence metric is used to evaluate the strength of the rules. Confidence measures how often the consequent item(s) appear in bills when the antecedent item(s) are present.\n",
    "        A minimum confidence threshold of 0.5 (50%) is set, meaning that only rules with a confidence greater than or equal to 0.5 will be considered significant.\n",
    "\n",
    "By applying these techniques to the pivot_table, the code enables the discovery of frequent itemsets and the extraction of meaningful association rules, helping to uncover hidden patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = rules.sort_values(['confidence', 'lift'], ascending =[False, False]) \n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules.sort_values(by='support', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort rules by support in descending order\n",
    "sorted_rules = rules.sort_values(by='support', ascending=False)\n",
    "\n",
    "# Calculate cumulative support\n",
    "cumulative_support = np.cumsum(sorted_rules['support'] / np.sum(sorted_rules['support']) * 100)\n",
    "\n",
    "# Bar plot for Support\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "ax1.bar(range(len(sorted_rules)), sorted_rules['support'], align='center')\n",
    "plt.xticks(range(len(sorted_rules)), ['' for _ in range(len(sorted_rules))])  # Remove x-axis labels\n",
    "ax1.set_xlabel('Association Rule')\n",
    "ax1.set_ylabel('Support')\n",
    "ax1.set_title('Support of Association Rules')\n",
    "\n",
    "# CDF plot for cumulative support\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(len(sorted_rules)), cumulative_support, color='#AA4A44', linestyle='--')\n",
    "ax2.set_ylabel('Cumulative Support (%)', c='#AA4A44')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot for Confidence vs. Support\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(rules['support'], rules['confidence'], alpha=0.4)\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Confidence vs. Support of Association Rules')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6789e",
   "metadata": {},
   "source": [
    "These two visualizations explore the association rules: a bar plot for the support of association rules and a scatter plot for the confidence vs. support of association rules.\n",
    "\n",
    "The bar plot represents the support values of the association rules. Each bar corresponds to a rule, and its height represents the support value, indicating how frequently the rule occurs in the dataset. The y-axis represents the support, while the x-axis does not display any labels, focusing solely on the visualization of support values.\n",
    "\n",
    "The cumulative distribution function (CDF) plot showcases the cumulative support of the association rules as a percentage. It helps understand the distribution of support values across the rules in a cumulative manner. The red dashed line in the CDF plot connects the cumulative support values for each rule, providing insights into the accumulation of support as the rules progress.\n",
    "\n",
    "The scatter plot displays the relationship between confidence and support for the association rules. Each point represents a rule, with the x-axis representing the support and the y-axis representing the confidence. The plot shows how the confidence varies with different levels of support, helping identify any patterns or trends between these two metrics.\n",
    "\n",
    "These visualizations offer valuable insights into the support, confidence, and their relationships within the association rules, aiding in the interpretation and analysis of the rules' strength and significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter association rules for cross-selling opportunities\n",
    "cross_selling_rules = rules[(rules['antecedents'].apply(len) == 1) & (rules['consequents'].apply(len) == 1)]\n",
    "\n",
    "# Sort rules based on confidence and support\n",
    "cross_selling_rules = cross_selling_rules.sort_values(by=['confidence', 'support'], ascending=False)\n",
    "\n",
    "# Select top cross-selling recommendations\n",
    "top_cross_selling = cross_selling_rules.head(5)\n",
    "\n",
    "# Filter association rules for upselling opportunities\n",
    "upselling_rules = rules[(rules['antecedents'].apply(len) == 1) & (rules['consequents'].apply(len) > 1)]\n",
    "\n",
    "# Sort rules based on confidence and support\n",
    "upselling_rules = upselling_rules.sort_values(by=['confidence', 'support'], ascending=False)\n",
    "\n",
    "# Select top upselling recommendations\n",
    "top_upselling = upselling_rules.head(5)\n",
    "\n",
    "# Display cross-selling recommendations\n",
    "print(\"Cross-Selling Recommendations:\")\n",
    "for idx, row in top_cross_selling.iterrows():\n",
    "    antecedent = list(row['antecedents'])[0]\n",
    "    consequent = list(row['consequents'])[0]\n",
    "    print(f\"Customers who bought '{antecedent}' also bought '{consequent}'.\")\n",
    "\n",
    "# Display upselling recommendations\n",
    "print(\"\\nUpselling Recommendations:\")\n",
    "for idx, row in top_upselling.iterrows():\n",
    "    antecedent = list(row['antecedents'])[0]\n",
    "    consequents = list(row['consequents'])\n",
    "    print(f\"For customers who bought '{antecedent}', recommend the following upgrades: {', '.join(consequents)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ffbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_upselling = upselling_rules.sort_values(['confidence', 'support'], ascending=False).drop_duplicates('antecedents')[:5]\n",
    "for idx, row in top_upselling.iterrows():\n",
    "    antecedent = list(row['antecedents'])[0]\n",
    "    consequents = list(row['consequents'])\n",
    "    print(f\"For customers who bought '{antecedent}', recommend the following upgrades: {', '.join(consequents)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
